{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy data in Pandas\n",
    "\n",
    "### (An opinionated guide to tabular data analysis in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`git clone --depth=1 git@github.com:ASPP/2019-camerino-tidy-data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter reminders:\n",
    "\n",
    "- IF IN DOUBT: Press Esc (enter command mode), then h (get all keyboard shortcuts)\n",
    "\n",
    "Some of the most useful commands:\n",
    "\n",
    "* Switching modes:\n",
    "    - Esc : get into Command mode (leaves Edit mode)\n",
    "    - Enter : edit a cell (puts you in Edit mode)\n",
    "\n",
    "- Ctrl+Enter : run the code in the current cell\n",
    "- Shift+Enter : run the code in the current cell AND advanced to next cell (works in any mode)\n",
    "\n",
    "* In command mode:\n",
    "    - h : see help (see all commands)\n",
    "    - a / b : add new cell above/below\n",
    "    - m : turn current cell into a Markdown cell\n",
    "    - y : turn current cell into a Code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get help on Python objects and functions with `help()` or the `?` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T11:29:49.600774Z",
     "start_time": "2019-01-19T11:29:49.581459Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T11:29:50.000633Z",
     "start_time": "2019-01-19T11:29:49.987260Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This causes Jupyter to display any matplotlib plots directly in the notebook\n",
    "# It also works for pandas and seaborn, since they use matplotlib to render plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T11:29:51.169379Z",
     "start_time": "2019-01-19T11:29:51.163406Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule(ish)\n",
    "\n",
    "- 16:00-16:45 basic pandas\n",
    "- 16:45-17:00 introduction to split-apply-combine\n",
    "- (break)\n",
    "- 17:30-18:00 split-apply-combine (ctd)\n",
    "- 18:00-18:30 plotting (matplotlib, seaborn, Altair)\n",
    "- 18:30-19:00 tidy data and how to get it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pandas üßºüêº"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the Pandas website and documentation at https://pandas.pydata.org/\n",
    "\n",
    "Pandas is a popular Python library for handling tabular data. It provides much of the same functionality for Python as do data frames in the R language. \n",
    "\n",
    "The fundamental data types in Pandas are a Series, representing a 1D array of data. Programmatically, a good way to think of a Series is of *a (1D) NumPy array* **plus** an *index*, allowing you to access elements not just by numeric order in the array, but also by the index (could be names, string IDs, timestamps, or even tuples). By default, series indices are numeric 0-indexed, just like NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"Diego\",\"Jessica\",\"Farah\"])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a DataFrame, representing a 2D table of data. Conceptually, a DataFrame is like a spreadsheet or a table in a database. Programmatically, a good way to think of DataFrames is a *dictionary* mapping *column names* to *pandas Series*. That is indeed how to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Name': [\"Diego\",\"Jessica\",\"Farah\"],\n",
    "                   'Age': [34, 27, 50]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important constraint on the Series of a DataFrame: they must all share the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index = 476, 839, 234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = pd.Series([34, 27, 50])\n",
    "ages.index = 839, 234, 998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'name': s, 'age': ages})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of a DataFrame [has a particular type](https://pbpython.com/pandas_dtypes.html) (ints, floats, datetimes, strings etc).\n",
    "\n",
    "Above, we constructed data manually using lists and dictionaries, to help us understand the pandas data model. For the rest of this workshop, we will work with real data, which mostly means reading from CSV files or Excel files. (But note the existence of [Apache Parquet](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html) for a much more efficient, fast, and data-preserving alternative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use for this section is from the [Long Term Evolution Experiment (LTEE)](http://myxo.css.msu.edu/ecoli/). This experiment has been running for over 30 years (Feb 1988) and over 50,000 E. coli generations, and is still ongoing. It thus firmly belongs in the annals of badass experiments in science.\n",
    "\n",
    "Twelve separate populations of E. coli have been propagated for the life of the experiment. Every 500 generations, each population is cloned and stored, allowing researchers to study evolutionary behaviour over the long term, and to \"rewind and replay\" alternate evolutionary trajectories by propagating from an earlier clone. \n",
    "\n",
    "Several interesting events have occurred during the experiment. Some populations have spontaneously developed hypermutator phenotypes. In addition, around generation 31,000 one population, Ara-3, spontaneously developed a rare and novel Cit+ mutation, giving it the ability to metabolise citrate in the substrate.\n",
    "\n",
    "There have been many publications from this experiment. A handful:\n",
    "\n",
    "- [Blount et al 2008: Historical contingency and the evolution of a key innovation in an experimental population of Escherichia coli](https://www.pnas.org/content/105/23/7899) - on the spontaneous development of citrate metabolisation and on potentiating mutations\n",
    "- [Tenaillon et al 2016: Tempo and mode of genome evolution in a 50,000-generation experiment](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4988878) - various investigations by sequencing and variant-calling over 50,000 generations of clones, including discussion of hypermutator phenotypes and genetic drift vs natural selection.\n",
    "\n",
    "Sequence data from clones is available, but for this workshop we'll be using some published tabular data.\n",
    "\n",
    "A version of this dataset is also used by the [Data Carpentry lessons on Genomics](https://datacarpentry.org/genomics-workshop/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the repository you'll find the files:\n",
    "\n",
    "- ltee_sampleruns.csv : sample and sequencing run metadata for the E. coli clones\n",
    "- ltee_mutations.csv : analysis output from variant calling on the E. coli clones\n",
    "- ltee_relative_fitness.tsv : relative fitness values for each population and generation up to generation 10,000\n",
    "- ltee_cell_size.tsv : cell sizes for each population and generation up to 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the sample and run metadata. Pandas has functions for reading in many data types. Try looking at the documentation for `read_csv()` by running `help(pd.read_csv)` or `pd.read_csv?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns = pd.read_csv('data/ltee_sampleruns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows and columns\n",
    "sampleruns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The first few rows\n",
    "sampleruns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column data types\n",
    "sampleruns.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "sampleruns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index (i.e. row names)\n",
    "# In this case we didn't provide an index and rows have simply been numbered for us by Pandas\n",
    "sampleruns.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "1. Use `pd.read_csv()` to read the file `ltee_mutations.csv` into a variable called `mutations`.\n",
    "2. Check the column headings and the number of rows in this dataset, and have a look at the first few rows. Compare the size of the dataset and the variables to `sampleruns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it.\n",
    "%load pandas_and_tidying_ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a dictionary, we can extract a column from the DataFrame by indexing with square brackets, e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a column\n",
    "sampleruns['Strain ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what is the type of `sampleruns['Strain ID']`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's set our index (row names) to something more meaningful to make it easier to see what's going on. The Strain ID uniquely identifies each sample, so it is probably a sensible index. We can use `sampleruns.set_index()`, or we can assign to the index directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns.index = sampleruns['Strain ID']\n",
    "sampleruns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important ways to extract data from a DataFrame are `loc` and `iloc`. `loc` uses the index and the column names; `iloc` uses the row and column numbers, counting from zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns.loc['REL768B', 'Accession']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row 0, column 9\n",
    "sampleruns.iloc[0, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 1-3, column 9\n",
    "sampleruns.iloc[1:4, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All columns\n",
    "sampleruns.loc['REL768B', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists let us specify any set of rows and columns, in any order\n",
    "sampleruns.loc[['REL768A','REL958A'], ['Read Type', 'Read Length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use True/False values to perform boolean indexing. \n",
    "# Pandas will return the rows/columns matching the True values we pass in.\n",
    "# This will be useful later for filtering data\n",
    "iris.loc[0:5, [True, False, True, True, False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "1. Set the index of `mutations` to be the same as the \"Strain ID\" column.\n",
    "2. Extract the population, generation, and number of total mutations for strain REL11345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it\n",
    "%load pandas_and_tidying_ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single column of a DataFrame is a Series object. Series have a data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Sequencing Depth'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a DataFrame, a Series has an index. In this case we got our Series from a column of a DataFrame, so it will have the same index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Sequencing Depth'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several convenience functions defined on Series. For instance, we can find the average sequencing depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Sequencing Depth'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for numeric variables we have, for instance, `.min()` and `.max()`, `.median()`, `std()`, and `sum()`.\n",
    "\n",
    "`.describe()` is a convenience function for getting several summary statistics at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Sequencing Depth'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-numeric variable types such as strings and categoricals, we may want to look at the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Read Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns['Read Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas makes use of numpy vectorisation, meaning we can do operations on Series with simple syntax, and it will be efficient to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 500 generations takes 75 days\n",
    "mutations['Days'] = mutations['Generation'] * 0.15\n",
    "mutations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations['Population'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mutations['Total Deleted Base Pairs'] + mutations['Total Inserted Base Pairs']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the `Analysis Notes` column contains a lot of NaN's. This means \"not a number\" and represents a missing value - i.e. these cells are empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which values are missing with `.isnull()`. This converts every value in the DataFrame (or Series) into a boolean True/False value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sampleruns.isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding up booleans will treat `True` as `1` and `False` as `0`. A common approach is to use `sum()` to count how many `True` values there are. So we can count missing values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleruns.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sampleruns` had 264 rows, so it looks like there are a few non-empty note cells. We could count this explicitly by taking the logical `not` of our True/False values, i.e. adding up cells where `isnull()` is `False`. For manipulating array-like data, we can't use the `not`, `and` and `or` boolean operators. Instead we need to use the bitwise operators `~`, `&`, and `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~sampleruns.isnull()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Pandas summed each column. We can use `sum(axis=1)` to override this default and sum each row instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort on a field, or list of fields, with `.sort_values()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get a random example subset\n",
    "subset = sampleruns.sample(15)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset.sort_values('Generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset.sort_values(['Population','Generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter datasets using boolean indexing. This means that if we use a logical expression produce a boolean Series with a logical expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['Population'] == 'Ara+5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then select out only the rows (or sometimes columns) where that logical expression is True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset.loc[subset['Population'] == 'Ara+5', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "1. Filter the `sampleruns` dataset to extract only rows which contain Analysis Notes, i.e. those where this field is not empty.\n",
    "2. Filter the `mutations` dataset to extract only samples with more than 1500 total mutations. \n",
    "3. Sort the resulting data from (2) by the number of \"Small Indels\". Have a look at the resulting Population and Generation columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it\n",
    "%load pandas_and_tidying_ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read in the classic `iris` and `mtcars` datasets for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('data/iris.csv')\n",
    "cars = pd.read_csv('data/mtcars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge two datasets together by matching corresponding variables.\n",
    "\n",
    "Our main options are `DataFrame.join()` and `pandas.merge()`. `merge()` is a little more flexible, so we'll demonstrate that.\n",
    "\n",
    "Recall the `cars` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `origin` column here, which is a number 1-3, is actually intended to represent the country of origin. It's encoded as:\n",
    "\n",
    "- USA : 1\n",
    "- Europe : 2\n",
    "- Japan : 3\n",
    "\n",
    "Let's make a DataFrame to represent this mapping. We'll add a fourth code for Australia, which doesn't appear in the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_codes = pd.DataFrame(\n",
    "    {'origin': [1, 2, 3],\n",
    "     'origin_country': ['USA', 'Europe', 'Japan']}\n",
    ")\n",
    "\n",
    "origin_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pandas.merge()` to join our `cars` table to our `origin_codes` table using the shared `origin` field, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_ori = pd.merge(cars, origin_codes)\n",
    "cars_ori.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"just worked\" because Pandas correctly deduced that the identically-named field(s) were the ones to match on. Sometimes we might need to be more verbose. In this case, this accomplishes the same thing as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use origin from the left dataframe (cars) and from the right (origin_codes)\n",
    "# use how=\"left\" (keep all origin values that exist in the left dataframe)\n",
    "cars_ori = pd.merge(\n",
    "    cars,\n",
    "    origin_codes,\n",
    "    left_on='origin',\n",
    "    right_on='origin',\n",
    "    how='left',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Try to merge our `sampleinfo` and `mutations` columns. This time there are three shared fields: 'Strain ID', 'Population', and 'Generation'. (*Hint:* although only 'Strain ID' is needed to uniquely identify  rows, we want to specify all matching variables so that Pandas knows to only include each of these variables once in the resulting dataframe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it\n",
    "%load pandas_and_tidying_ex4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1170/1*w2oGdXv5btEMxAkAsz8fbg.png\" />\n",
    "\n",
    "(Source: https://medium.com/analytics-vidhya/split-apply-combine-strategy-for-data-mining-4fd6e2a0cc99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.groupby('cylinders')['mpg'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Now that the metadata on Mutator phenotypes is together with the information on actual mutations, we can try exploring the relationships between these fields. What is the average number of mutations for each mutator phenotype?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it\n",
    "%load pandas_and_tidying_ex5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off, let's write out our tidied and merged table to a new file, for future analyses. To write to CSV, we can use the `to_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't keep the index as we still have the Strain ID column\n",
    "ltee.to_csv('ltee_solution.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.read_csv('data/publishers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Stuart Lawson, [Subscription costs FOIs](https://figshare.com/articles/Journal_subscription_costs_FOIs_to_UK_universities/1186832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Explore the following questions using subsetting and split-apply-combine:\n",
    "\n",
    "- what was the total cost of publishing to UK research (among public universities) in the years 2010-2016?\n",
    "- what was the total cost of publishing to UK research in 2016?\n",
    "- How much did universities pay to Elsevier in each of the years 2010-2016?\n",
    "- *(Advanced)* Use a [named aggregation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#named-aggregation) to find the min/max/mean/median that universities paid to Elsevier in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... then write your solution into this file and load it\n",
    "%load pandas_and_tidying_ex6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why visualize data\n",
    "\n",
    "- Exploration\n",
    "- Exposition\n",
    "- Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(source: https://www.autodeskresearch.com/publications/samestats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_images/viz.png\" width=250/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| A     | B      |\n",
    "|-------|--------|\n",
    "| 6.61  | 16.81  |\n",
    "| 7.89  | 16.84  |\n",
    "| 10.54 | 18.96  |\n",
    "| 11.30 | 18.98  |\n",
    "| 12.27 | 20.96  |\n",
    "| 12.27 | 20.97  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_images/encoding.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical point encodings\n",
    "\n",
    "<img src=\"https://media.springernature.com/m685/nature-assets/nmeth/journal/v10/n6/images/nmeth.2490-F2.jpg\" />\n",
    "\n",
    "Source: Krzywinski & Wong, Plotting symbols, Nature Methods 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestalt principles\n",
    "\n",
    "http://emeeks.github.io/gestaltdataviz/section1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity\n",
    "\n",
    "![Great wall](_images/great-wall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity and similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asteroid belt](_images/asteroids.png)\n",
    "\n",
    "Source: Jake VanderPlas, 2013 John Hunter Excellence in Plotting competition, 3rd place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we're going to load in two main datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House sales data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains information about house sales in Seattle in 2014 and 2015. We have information on the houses themselves: location, size, quality, view, and whether the house is tagged as 'waterfront' or not. We also have information on the date and price of each sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T23:56:53.195524Z",
     "start_time": "2018-01-14T23:56:53.140978Z"
    }
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\n",
    "    'data/housing-data-10000.csv', \n",
    "    usecols=['id','date','price','lat','long', 'zipcode',\n",
    "             'waterfront','view','grade','sqft_living'],\n",
    "    parse_dates=['date'], \n",
    "    dtype={'zipcode': 'category',\n",
    "           'waterfront': 'bool'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib is the oldest and still the fundamental plotting library in Python. It has a huge range of capabilities. Many other libraries (including Seaborn) use Matplotlib as a back-end renderer.\n",
    "\n",
    "Today we're focussing on plotting tabular data. We won't touch on all Matplotlib's capabilities. If you want to see more of the range of things Matplotlib can do, you can look through the [Matplotlib gallery](https://matplotlib.org/gallery.html.), or try out this excellent [Matplotlib tutorial](https://www.labri.fr/perso/nrougier/teaching/matplotlib/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Matplotlib plot with legend and annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T05:36:41.975324Z",
     "start_time": "2018-01-14T05:36:41.805467Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]\n",
    "y = [2,5,10,17,26]\n",
    "y2 = [1,4,9,11,9]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, c='blue', label='Projected')\n",
    "ax.scatter(x, y2, c='red', label='Actual')\n",
    "fig.legend()\n",
    "ax.annotate(\n",
    "    'where it all went wrong', \n",
    "    xy=(3,10),\n",
    "    xytext=(1,12),\n",
    "    arrowprops={'width': 2}\n",
    ")\n",
    "\n",
    "fig.savefig('example_matplotlib.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn builds on Matplotlib. Some nice features are:\n",
    "\n",
    "- works directly with Pandas dataframes, concise syntax\n",
    "- lots of plot types, including some more advanced options\n",
    "- statistical plotting: many plots do summary statistics for you\n",
    "- good default aesthetics and easy control of aesthetics\n",
    "- uses Matplotlib, so can use all Matplotlib backends (incl lots of image file formats)\n",
    "- underlying Matplotlib objects can be tweaked directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, here's the plot we made before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Time': [1,2,3,4,5],\n",
    "    'Projected': [2,5,10,17,26],\n",
    "    'Actual': [1,4,9,11,9]\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=df, x='Time', y='Actual', color='red', ax=ax)\n",
    "sns.lineplot(data=df, x='Time', y='Projected', color='blue', ax=ax)\n",
    "\n",
    "ax.annotate(\n",
    "    'where it all went wrong', \n",
    "    xy=(3,10),\n",
    "    xytext=(1,12),\n",
    "    arrowprops={'width': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can add changes like annotations in exactly the same way, as we have Matplotlib Figure and Axes objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn and Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases we can use Seaborn by passing in lists (or arrays or series) directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T23:56:54.489607Z",
     "start_time": "2018-01-14T23:56:53.283680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=['A','B','C'], y=[33,44,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However Seaborn is aware of Pandas and it is very common to use Seaborn directly with DataFrames. Plotting functions can take a DataFrame as their `data` parameter and then refer directly to column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T23:56:54.706924Z",
     "start_time": "2018-01-14T23:56:54.492344Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=iris, x='species', y='petal_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Seaborn has interpreted the `x` and `y` arguments as field names in the supplied DataFrame. Notice also that Seaborn has performed the summary statistics for us - in this case, using the default `estimator`, which is `mean()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also what happens if we simply swap the `x` and `y` parameters. Seaborn will automatically deduce that the categorical or string-like variable must be the bar labels, and the numeric variable must be the numeric axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T23:56:54.706924Z",
     "start_time": "2018-01-14T23:56:54.492344Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(data=iris, y='species', x='petal_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the grouping is not obviously categorical? How could we fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=cars, x='acceleration', y='origin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: \n",
    "\n",
    "**1:** Create a count plot using `sns.countplot()` on the `ltee` data, showing how many clones have each `Mutator` phenotype. Note that you do not need to specify the `y` axis variable for a countplot, just the `x` axis variable (i.e. category).\n",
    "\n",
    "**2:** Create a (vertical) bar plot using the `sales` data, showing how house prices vary with the value of the property `grade`.\n",
    "\n",
    "Bar plots are often deplored as a way of showing statistical estimates, as only the top of the bar is really important, and the bar itself is a visual distraction. A point plot is an alternative, and plots like box plots can show more information. Several other plot types also show distributional information within categories.\n",
    "\n",
    "**3:** Reproduce the plot you just made, using instead each of the Seaborn functions:\n",
    "\n",
    "- pointplot()\n",
    "- boxplot()\n",
    "- stripplot() [SEE WARNING]  (try the `jitter` parameter)\n",
    "- swarmplot() [SEE WARNING]\n",
    "\n",
    "Note what sort of information about the distribution is shown by each.\n",
    "\n",
    "WARNING: `stripplot()` and `swarmplot()` will plot individual data points. There are too many house sales to easily display in this way - you should subsample the dataframe with e.g.  `data=sales.sample(100)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many Seaborn plotting functions take a `hue` parameter. This colours the plot elements by some categorical variable, but more than this, summary statistics are calculated for each level of the hue variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=ltee, \n",
    "                x='Generation', \n",
    "                y='Synonymous Base Substitutions', \n",
    "                palette='bright') # hue='Population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=ltee, \n",
    "             x='Generation', \n",
    "             y='Synonymous Base Substitutions', \n",
    "             palette='bright') # hue='Population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "- Using the subscription costs dataset (`subs`), plot the costs over time for each publisher, using the \"hue\" parameter to \"lineplot\" or \"barplot\".\n",
    "- Use the underlying matplotlib axes object and it's `set_scale` method to set a log scale on the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colour and Palettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has good colour options. There are a few ways we could want to use access colours:\n",
    "\n",
    "* Specify an individual colour for some plot element. Matplotlib named colours can be used, or rgb values specified. Also check out the `sns.xkcd_rgb` dictionary of 954 named colours from the XKCD colour survey - for instance, `sns.xkcd['fire engine red']` is a colour.\n",
    "* Specify a colormap, for mapping a continuous value to colour. All Matplotlib colormaps can be used by name. You can see these under the `plt.cm` module. Seaborn's `light_palette()` and `dark_palette()` functions can also generate custom colourmaps easily.\n",
    "* Specify a discrete colour palette (a list of colours), for mapping a discrete or categorical variable to colour. In Seaborn, there is a distinction between colour palettes and colormaps. In general, you can create a colour palette by explicitly listing some colours, or by selecting a series of colours along some colormap. There are several functions, including `color_palette()`, `light_palette()`, `dark_palette()`, `diverging_palette()` and `xkcd_palette()`, which can produce many discrete colour palettes of whatever size you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:00:08.929020Z",
     "start_time": "2018-01-15T00:00:08.834779Z"
    }
   },
   "outputs": [],
   "source": [
    "# An example discrete colour palette of 7 colours, based on the XKCD colour \"denim blue\"\n",
    "# palplot is a function to visualise a palette\n",
    "palette = sns.light_palette(\"denim blue\", n_colors=7, input='xkcd')\n",
    "sns.palplot(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:00:09.043908Z",
     "start_time": "2018-01-15T00:00:08.932500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Equivalently (to illustrate that we can use an rgb value directly)\n",
    "denim_blue = sns.xkcd_rgb[\"denim blue\"]\n",
    "print(denim_blue)\n",
    "palette = sns.light_palette(denim_blue, n_colors=7)\n",
    "print(palette)\n",
    "sns.palplot(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altair is not bundled with Anaconda - you may need\n",
    "\n",
    "`conda install -c conda-forge altair vega`\n",
    "\n",
    "or\n",
    "\n",
    "`pip install altair vega`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and initialise altair\n",
    "import altair as alt\n",
    "# this line is needed in jupyter notebook, but not jupyter-lab\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTEE data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we'll use a large flat file containing both sample metadata on each clone, and information on observed mutations in their genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have done the pandas and data tidying workshop and saved the final file,\n",
    "# you can try reading that file in here if you prefer\n",
    "ltee = pd.read_csv(\n",
    "    'data/ltee_merged.csv', index_col='Strain ID'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House sales data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the house sales data from above, but we will subsample it for Altair, which is not (yet) designed for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssales = sales.sample(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Altair](https://altair-viz.github.io) is a library for creating interactive plots. \n",
    "\n",
    "Altair is built around the [Vega-Lite](https://vega.github.io/vega-lite/) schema, a \"visualisation grammar\". Altair plots are specified in Python, then converted behind the scenes to a declarative JSON structure that follows the Vega-Lite schema, which can then be rendered by a Javascript library.\n",
    "\n",
    "Altair works very well with Pandas - in fact, it usually expects data to be in a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple interactive plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example of an Altair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'Time': [1,2,3,4,5],\n",
    "     'Projected': [2,5,10,17,26],\n",
    "     'Actual': [1,4,9,11,9]},\n",
    ")\n",
    "\n",
    "chart = alt.Chart(df)\n",
    "\n",
    "chart.mark_line(color='blue').encode(x='Time', y='Projected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mark_line()` tells Altair we want to draw a line, and `encode()` is used to assign our variables (columns of the DataFrame) to possible encoding channels of the line. We've just used `x` and `y`. Notice that we set the colour in `mark_line()`; if we'd wanted to set the colour to encode some variable, we would have set it in `encode()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple pan-and-zoom interactivity can be added with `.interactive()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(df)\n",
    "\n",
    "chart.mark_line(color='blue').encode(x='Time', y='Projected').interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to produce the chart we produced earlier, with scatter points, we can use `alt.layer()`, which takes the components as arguments and layers them on top of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T00:55:27.518001Z",
     "start_time": "2018-01-15T00:55:27.491207Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Time': [1,2,3,4,5],\n",
    "    'Projected': [2,5,10,17,26],\n",
    "    'Actual': [1,4,9,11,9],\n",
    "})\n",
    "\n",
    "chart = alt.Chart(df)\n",
    "line = chart.mark_line(color='blue').encode(\n",
    "    x='Time', y='Projected'\n",
    ")\n",
    "points = chart.mark_point(color='red').encode(\n",
    "    x='Time', y='Actual'\n",
    ")\n",
    "\n",
    "alt.layer(line, points).interactive()\n",
    "# or (line + points).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: \n",
    "\n",
    "Recreate, in Altair, the scatter plot of house sales with x-coordinates given by `longitude` and y-coordinates given by `latitude`. You can use `.mark_point()` or `.mark_circle()`. One issue you will discover, if you use the `x` and `y` encodings, is that Altair and Vega-lite include the axis zero by default. If you have time, do some googling to see how you can fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altair recognises four fundamental [data types](https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types):\n",
    "\n",
    "| Data Type | Shorthand Code | Description |\n",
    "|-|-|-|\n",
    "| quantitative | Q | a continuous real-valued quantity |\n",
    "| ordinal | O | a discrete ordered quantity |\n",
    "| nominal | N | a discrete unordered category |\n",
    "| temporal | T | a time or date value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(alt.Chart(ssales.sample(10))\n",
    "    .mark_bar()\n",
    "    .encode(x='date:N', y='price:Q')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What kind of sequencing experiments have been done on the LTEE data?\n",
    "\n",
    "chart = alt.Chart(data=ltee)\n",
    "\n",
    "(chart.mark_point()\n",
    "    .encode(x=alt.X('Sequencing Depth', \n",
    "                    scale=alt.Scale(type='log')),\n",
    "            y='Read Type',\n",
    "            color='Read Length:N')  # try N, O, Q\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10:\n",
    "\n",
    "* Colour your \"geographical\" scatter plot of house sales according to whether the property is or is not a waterfront property. What is the appropriate data type?\n",
    "* Colour according to price (if you prefer, try a log scale). What is the appropriate data type?\n",
    "* Colour according to view. What is the appropriate data type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks and encodings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altair has various marks used to draw different kinds of plots. For instance:\n",
    "\n",
    "* `mark_point()` : points on a scatter plot\n",
    "* `mark_bar()` : rectangular bars, in e.g. a bar plot or histogram\n",
    "* `mark_area()` : filled/shaded areas\n",
    "\n",
    "The full list of marks can be found at [https://altair-viz.github.io/user_guide/marks.html](https://altair-viz.github.io/user_guide/marks.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign variables of the DataFrame to different encoding channels. For instance:\n",
    "\n",
    "* `x` and `y` : position of mark (scatterpoint position, top of bar-chart bar, etc)\n",
    "* `color` : colour of the mark (colour of point, bar, shaded area etc)\n",
    "* `size` : size of the mark (point size, bar width, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a list of encodings at [https://altair-viz.github.io/user_guide/encoding.html](https://altair-viz.github.io/user_guide/encoding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: marks from common base\n",
    "base = (\n",
    "    alt.Chart(ssales.sample(10))\n",
    "       .encode(x='date:T', y='price:Q')\n",
    ")\n",
    "\n",
    "alt.hconcat(base.mark_point(color='red'), base.mark_bar())\n",
    "# or base.mark_point(color='red') | base.mark_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: box plot\n",
    "base = alt.Chart(ltee)\n",
    "\n",
    "base.mark_boxplot().encode(\n",
    "    x='Population', y='Total Mutations'\n",
    ")\n",
    "# Log scale works here\n",
    "# Color also works here (sort of!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover text can be added to a plot by setting the `tooltip` encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: \n",
    "\n",
    "On your scatter plot of house location, set the hover info to display the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic pan-and-zoom interactivity is set with `.interactive()`. This is actually short for:\n",
    "\n",
    "```\n",
    "chart.add_selection(\n",
    "    alt.selection_interval(bind='scales')\n",
    ")\n",
    "```\n",
    "\n",
    "This means we've added a behaviour where dragging the mouse (selecting an interval) causes the plot to zoom and pan (bind='scales'). We can create custom interactive behaviour with [bindings, selections, and conditions](https://altair-viz.github.io/user_guide/interactions.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy data\n",
    "\n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png\" />\n",
    "\n",
    "(source: https://r4ds.had.co.nz/tidy-data.html#fig:tidy-structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the \"house-sales\" excel spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape data using particular columns, with `melt` and `pivot` or `pivot_table`. We'll have a look at this below.\n",
    "\n",
    "We can also reshape data using the column names and index, with `stack` and `unstack`. This requires MultiIndexes, which we won't go into today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two tiny \"wide\" datasets based on our \"untidy\" housing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T11:51:16.434295Z",
     "start_time": "2019-01-19T11:51:16.254462Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sales_wide1 = pd.read_excel(\n",
    "    'house-sales.xlsx',\n",
    "    sheet_name='untidy data',\n",
    "    header=0,\n",
    "    usecols='B:J',\n",
    "    index_col=0,\n",
    "    skiprows=list(range(4)),\n",
    "    nrows=3,\n",
    "    parse_dates=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Excel has already stuck its grimy fingers into the data integrity pie, and converted some text to dates, inaccurately in this case. The only way to fix this is manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1['sale date 1'] = pd.to_datetime(['2014-10-16', '2014-09-24', '2014-12-09'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1['sale date 2'] = pd.to_datetime([None, '2014-12-15', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "\n",
    "- Use `pd.read_excel` as above to read the 2015 house sales table into a DataFrame.\n",
    "- Ensure the sale dates are accurate datetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_wide2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide2['sale date 1'] = pd.to_datetime(sales_wide2['sale date 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide2['sale date 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide2['sale date 2'] = pd.to_datetime([None, '2015-04-30', None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we reshape these into tidy form? \n",
    "\n",
    "The Pandas `melt` function will do this. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_wide1.melt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has put every variable (i.e. every column) into the new `variable` column. This probably isn't what we want. It's only the price columns that are \"wide\", the other variables were fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain columns property_id and bedrooms\n",
    "sales_wide1.melt(id_vars=['property_id','bedrooms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting close to what we want. The `variable` column contains the original column names and tells us whether the price we're looking at was from the first or second sale (this may or may not be information we care about). The `value` column contains values in the melted columns, i.e. the actual price. \n",
    "\n",
    "Now we technically have long form and have eliminated the duplicated `price` variable; all prices are now in the `value` column. Notice that properties can now appear more than once in the table; conceptually, we have a row per sale rather than a row per property. \n",
    "\n",
    "We can tell `melt()` what to call the `variable` and `value` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_tidy = sales_wide1.melt(\n",
    "    id_vars=['property_id','bedrooms'], \n",
    "    var_name='sale_number',\n",
    "    value_name='price',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have empty extra rows where there was no sale in the original table, i.e. rows 3 and 5. We could use `dropna()` to get rid of these. A more generic approach would be to use filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales_wide1.melt(\n",
    "    id_vars=['property_id','bedrooms'], \n",
    "    var_name='sale_number',\n",
    "    value_name='price')\n",
    "sales = sales[~sales['price'].isnull()]\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty good! Now we could run commands like `sales[\"price\"].mean()` and get a sensible answer. We'll also be able to use the data easily to produce plots.\n",
    "\n",
    "If you want a challenge, think about how you could convert `sales_wide2` to tidy form - it's a fair bit harder.\n",
    "\n",
    "The inverse operation to `.melt()` is `.pivot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.pivot(index='property_id', columns='sale_number',\n",
    "            values='price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "\n",
    "The original Excel spreadsheets from the journal subscription costs tidy table are in:\n",
    "\n",
    "- Journal_publishing_cost_FOIs_UK_universities.xlsx\n",
    "- Journalsubscost20152016v6.xlsx\n",
    "\n",
    "This is the raw data as it was input by the researchers.\n",
    "\n",
    "- Is it tidy? (Hint: see second part of the exercise ;)\n",
    "- Use pandas.read_excel, pandas.DataFrame.melt, and pandas.concat to create a complete (2010-2016) tidy dataset from the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
